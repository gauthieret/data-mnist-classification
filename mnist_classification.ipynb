{"cells":[{"cell_type":"markdown","metadata":{"id":"LCykUEWw3DO-"},"source":["# MNIST Classification"]},{"cell_type":"markdown","metadata":{"id":"bCD7uCx73DPC"},"source":["üéØ <b><u>Exercise objectives</u></b>\n","- Understand the *MNIST* dataset \n","- Design your first **Convolutional Neural Network** (*CNN*) and answer questions such as:\n","    - what are *Convolutional Layers*? \n","    - how many *parameters* are involved in such a layer?\n","- Train this CNN on images"]},{"cell_type":"markdown","metadata":{"id":"mhJ-r1rg3DPD"},"source":["üöÄ <b><u>Let's get started!</u></b>\n","\n","Imagine that we are  back in time into the 90's.\n","You work at a *Post Office* and you have to deal with an enormous amount of letters on a daily basis. How could you automate the process of reading the ZIP Codes, which are a combination of 5 handwritten digits? \n","\n","This task, called the **Handwriting Recognition**, used to be a very complex problem back in those days. It was solved by *Bell Labs* (among others) where one of the Deep Learning gurus, [*Yann Le Cun*](https://en.wikipedia.org/wiki/Yann_LeCun), used to work.\n","\n","From [Wikipedia](https://en.wikipedia.org/wiki/Handwriting_recognition):\n","\n","> Handwriting recognition (HWR), also known as Handwritten Text Recognition (HTR), is the ability of a computer to receive and interpret intelligible handwritten input from sources such as paper documents, photographs, touch-screens and other devices"]},{"cell_type":"markdown","metadata":{"id":"uSbPN-MP3DPE"},"source":["![Number recognition](recognition.gif)\n","\n","*Note: The animation above is just here to help you visualize what happens with the different images: <br/> $\\rightarrow$ For each image, once the CNN is trained, it will predict what digit is written. The inputs are the different digits and not one animation/video!*"]},{"cell_type":"markdown","metadata":{"id":"T7WXeaI73DPE"},"source":["ü§î <b><u>How does this CNN work ?</u></b>\n","\n","- *Inputs*: Images (_each image shows a handwritten digit_)\n","- *Target*: For each image, you want your CNN model to predict the correct digit (between 0 and 9)\n","    - It is a **multi-class classification** task (more precisely a 10-class classification task since there are 10 different digits).\n","\n","üî¢ To improve the capacity of the Convolutional Neural Network to read these numbers, we need to feed it with many images representing handwritten digits. This is why the üìö [**MNIST dataset**](http://yann.lecun.com/exdb/mnist/) *(Mixed National Institute of Standards and Technology)* was created."]},{"cell_type":"markdown","metadata":{"id":"1e51gaEa3DPH"},"source":["## (1) The `MNIST` Dataset"]},{"cell_type":"code","execution_count":55,"metadata":{"id":"5bgxyla73DPF","executionInfo":{"status":"ok","timestamp":1668597203175,"user_tz":-60,"elapsed":937,"user":{"displayName":"Gauthier Etien","userId":"01301474931245940039"}}},"outputs":[],"source":["import numpy as np\n","import matplotlib.pyplot as plt"]},{"cell_type":"code","execution_count":56,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ySKN8Yev3DPI","executionInfo":{"status":"ok","timestamp":1668597205258,"user_tz":-60,"elapsed":214,"user":{"displayName":"Gauthier Etien","userId":"01301474931245940039"}},"outputId":"9f9172ad-6b95-4e26-92a0-65bf1594880e"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["(((60000, 28, 28), (60000,)), ((10000, 28, 28), (10000,)))"]},"metadata":{},"execution_count":56}],"source":["from tensorflow.keras import datasets\n","\n","\n","# Loading the MNIST Dataset...\n","(X_train, y_train), (X_test, y_test) = datasets.mnist.load_data(path=\"mnist.npz\")\n","\n","# The train set contains 60 000 images, each of them of size 28x28\n","# The test set contains 10 000 images, each of them of size 28x28\n","(X_train.shape, y_train.shape), (X_test.shape, y_test.shape)"]},{"cell_type":"code","source":["len(datasets.mnist.load_data(path=\"mnist.npz\"))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"467AMHPB70sK","executionInfo":{"status":"ok","timestamp":1668597207957,"user_tz":-60,"elapsed":353,"user":{"displayName":"Gauthier Etien","userId":"01301474931245940039"}},"outputId":"59f5951b-0236-4a8d-9d52-34adcb49bd4a"},"execution_count":57,"outputs":[{"output_type":"execute_result","data":{"text/plain":["2"]},"metadata":{},"execution_count":57}]},{"cell_type":"markdown","metadata":{"id":"wvv0e6Xw3DPH"},"source":["üìö Tensorflow/Keras offers multiple [**datasets**](https://www.tensorflow.org/api_docs/python/tf/keras/datasets) to play with:\n","- *Vectors*: `boston_housing` (regression)\n","- *Images* : `mnist`, `fashion_mnist`, `cifar10`, `cifar100` (classification)\n","- *Texts*: `imbd`, `reuters` (classification/sentiment analysis)\n","\n","\n","üíæ You can **load the MNIST dataset** with the following commands:"]},{"cell_type":"markdown","metadata":{"id":"DOkUzVna3DPJ"},"source":["### (1.1) Exploring the dataset"]},{"cell_type":"markdown","metadata":{"id":"Ez74YuHc3DPJ"},"source":["‚ùì **Question: Let's have look at some handwritten digits of this MNIST dataset.** ‚ùì\n","\n","üñ® Print some images from the *train set*.\n","\n","<details>\n","    <summary><i>Hints</i></summary>\n","\n","üí°*Hint*: use the `imshow` function from `matplotlib` with `cmap = \"gray\"`\n","\n","ü§® Note: if you don't specify this *cmap* argument, the weirdly displayed colors are just Matplotlib defaults...\n","    \n","</details>"]},{"cell_type":"code","execution_count":58,"metadata":{"tags":["challengify"],"colab":{"base_uri":"https://localhost:8080/","height":237},"id":"KD_fyRte3DPK","executionInfo":{"status":"ok","timestamp":1668597211760,"user_tz":-60,"elapsed":1535,"user":{"displayName":"Gauthier Etien","userId":"01301474931245940039"}},"outputId":"115e1b9f-b422-4abc-cd30-7399d0f6f478"},"outputs":[{"output_type":"stream","name":"stdout","text":["(60000, 28, 28)\n"]},{"output_type":"execute_result","data":{"text/plain":["(60000, 28, 28)"]},"metadata":{},"execution_count":58},{"output_type":"display_data","data":{"text/plain":["<Figure size 432x288 with 2 Axes>"],"image/png":"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\n"},"metadata":{"needs_background":"light"}}],"source":["# Let's load the famous MNIST dataset\n","from tensorflow.keras.datasets import mnist\n","(X_train, y_train), (X_test, y_test) = mnist.load_data()\n","X_train = X_train / 255.\n","X_test = X_test / 255.\n","\n","print(X_train.shape)\n","plt.subplot(1,2,1)\n","plt.imshow(X_train[5999], cmap=\"gray\");\n","plt.subplot(1,2,2)\n","plt.imshow(X_train[1], cmap=\"gray\");\n","(60000, 28, 28)"]},{"cell_type":"markdown","metadata":{"id":"NK__wvaR3DPK"},"source":["### (1.2) Image Preprocessing"]},{"cell_type":"markdown","metadata":{"id":"sE_SkHyw3DPK"},"source":["‚ùóÔ∏è **Neural Networks converge faster when the input data is somehow normalized** ‚ùóÔ∏è\n","\n","üë©üèª‚Äçüè´ How do we proceed for Convolutional Neural Networks ?\n","* The `RBG` intensities are coded between 0 and 255. \n","* We can simply divide the input data by the maximal value 255 to have all the pixels' intensities between 0 and 1 üòâ"]},{"cell_type":"markdown","metadata":{"id":"1IECG7_w3DPL"},"source":["‚ùì **Question ‚ùì As a first preprocessing step, please normalize your data.** \n","\n","Don't forget to do it both on your train data and your test data.\n","\n","(*Note: you can also center your data, by subtracting 0.5 from all the values, but it is not mandatory*)"]},{"cell_type":"code","execution_count":null,"metadata":{"tags":["challengify"],"id":"tU6IzmXt3DPL"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"11tCBuU93DPL"},"source":["### (1.3) Inputs' dimensionality"]},{"cell_type":"code","execution_count":59,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Az6n50BB3DPL","executionInfo":{"status":"ok","timestamp":1668597214712,"user_tz":-60,"elapsed":237,"user":{"displayName":"Gauthier Etien","userId":"01301474931245940039"}},"outputId":"b3f65ac7-2fc5-4081-af72-4644fa19818b"},"outputs":[{"output_type":"stream","name":"stdout","text":["(60000, 28, 28)\n","(10000, 28, 28)\n"]}],"source":["print(X_train.shape)\n","print(X_test.shape)"]},{"cell_type":"markdown","metadata":{"id":"KAyfK4f53DPN"},"source":["üëÜ Remember that you have 60,000 training images and 10,000 test images, each of size $(28, 28)$. However...\n","\n","> ‚ùóÔ∏è  **`Convolutional Neural Network models need to be fed with images whose last dimension is the number of channels`.**  \n","\n","> üßëüèª‚Äçüè´ The shape of tensors fed into ***ConvNets*** is the following: `(NUMBER_OF_IMAGES, HEIGHT, WIDTH, CHANNELS)`\n","\n","üïµüèªThis last dimension is clearly missing here. Can you guess the reason why?\n","<br>\n","<details>\n","    <summary><i>Answer<i></summary>\n","        \n","* All these $60000$ $ (28 \\times 28) $ pictures are black-and-white $ \\implies $ Each pixel lives on a spectrum from full black (0) to full white (1).\n","        \n","    * Theoretically, you don't need to know the number of channels for a black-and-white picture since there is only 1 channel (the \"whiteness\" of \"blackness\" of a pixel). However, it is still mandatory for the model to have this number of channels explicitly stated.\n","        \n","    * In comparison, colored pictures need multiple channels:\n","        - the RGB system with 3 channels (<b><span style=\"color:red\">Red</span> <span style=\"color:green\">Green</span> <span style=\"color:blue\">Blue</span></b>)\n","        - the CYMK system  with 4 channels (<b><span style=\"color:cyan\">Cyan</span> <span style=\"color:magenta\">Magenta</span> <span style=\"color:yellow\">Yellow</span> <span style=\"color:black\">Black</span></b>)\n","        \n","        \n","</details>        "]},{"cell_type":"markdown","metadata":{"id":"Z3pXWbDn3DPN"},"source":["‚ùì **Question: expanding dimensions** ‚ùì\n","\n","* Use the **`expand_dims`** to add one dimension at the end of the training data and test data.\n","\n","* Then, print the shapes of `X_train` and `X_test`. They should respectively be equal to $(60000, 28, 28, 1)$ and $(10000, 28, 28, 1)$."]},{"cell_type":"code","execution_count":60,"metadata":{"id":"mZcGyiwj3DPN","executionInfo":{"status":"ok","timestamp":1668597220731,"user_tz":-60,"elapsed":3,"user":{"displayName":"Gauthier Etien","userId":"01301474931245940039"}}},"outputs":[],"source":["from tensorflow.keras.backend import expand_dims"]},{"cell_type":"code","execution_count":61,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qQYzG7Sg3DPO","executionInfo":{"status":"ok","timestamp":1668597222653,"user_tz":-60,"elapsed":4,"user":{"displayName":"Gauthier Etien","userId":"01301474931245940039"}},"outputId":"71211e8d-7e6b-4a18-939d-c101d4f8cb95"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["(60000, 28, 28, 1)"]},"metadata":{},"execution_count":61}],"source":["X_train = X_train.reshape(len(X_train), 28, 28, 1)\n","X_test = X_test.reshape(len(X_test), 28, 28, 1)\n","X_train.shape\n"]},{"cell_type":"markdown","metadata":{"id":"Fdk0mDan3DPO"},"source":["### (1.4) Target encoding"]},{"cell_type":"markdown","metadata":{"id":"P8g2NB2u3DPO"},"source":["One more thing to for a multiclass classification task in Deep Leaning:\n","\n","üëâ _\"one-hot-encode\" the categories*_\n","\n","‚ùì **Question: encoding the labels** ‚ùì \n","\n","* Use **`to_categorical`** to transform your labels. \n","* Store the results into two variables that you can call **`y_train_cat`** and **`y_test_cat`**."]},{"cell_type":"code","execution_count":62,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fsOR8LZ-3DPO","executionInfo":{"status":"ok","timestamp":1668597225262,"user_tz":-60,"elapsed":226,"user":{"displayName":"Gauthier Etien","userId":"01301474931245940039"}},"outputId":"1477e2d8-8a92-44de-c6ca-c37be6b24fda"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["(10000, 10)"]},"metadata":{},"execution_count":62}],"source":["from tensorflow.keras.utils import to_categorical\n","\n","y_train_cat = to_categorical(y_train, num_classes=10)\n","y_test_cat = to_categorical(y_test, num_classes=10)\n","y_train_cat.shape\n","y_test_cat.shape\n"]},{"cell_type":"code","execution_count":63,"metadata":{"id":"SHsvBvaF3DPP","executionInfo":{"status":"ok","timestamp":1668597229494,"user_tz":-60,"elapsed":221,"user":{"displayName":"Gauthier Etien","userId":"01301474931245940039"}}},"outputs":[],"source":["# Quick check that you correctly used to_categorical\n","assert(y_train_cat.shape == (60000,10))\n","assert(y_test_cat.shape == (10000,10))"]},{"cell_type":"markdown","metadata":{"id":"BEKeol8e3DPP"},"source":["The data is now ready to be used. ‚úÖ"]},{"cell_type":"markdown","metadata":{"id":"fFwnn3oa3DPP"},"source":["## (2) The Convolutional Neural Network"]},{"cell_type":"markdown","metadata":{"id":"hPd8cAfb3DPP"},"source":["### (2.1) Architecture and compilation of a CNN"]},{"cell_type":"markdown","metadata":{"id":"DTrKn7ot3DPP"},"source":["\n","‚ùì **Question: CNN Architecture and compilation** ‚ùì\n","\n","Now, let's build a <u>Convolutional Neural Network</u> that has: \n","\n","\n","- a `Conv2D` layer with 8 filters, each of size $(4, 4)$, an input shape suitable for your task, the `relu` activation function, and `padding='same'`\n","- a `MaxPool2D` layer with a `pool_size` equal to $(2, 2)$\n","- a second `Conv2D` layer with 16 filters, each of size $(3, 3)$, and the `relu` activation function\n","- a second `MaxPool2D` layer with a `pool_size` equal to $(2, 2)$\n","\n","\n","- a `Flatten` layer\n","- a first `Dense` layer with 10 neurons and the `relu` activation function\n","- a last (predictive) layer that is suited for your task\n","\n","In the function that initializes this model, do not forget to include the <u>compilation of the model</u>, which:\n","* optimizes the `categorical_crossentropy` loss function,\n","* with the `adam` optimizer, \n","* and the `accuracy` as the metrics\n","\n","(*Note: you could add more classification metrics if you want but the dataset is well balanced!*)"]},{"cell_type":"code","execution_count":79,"metadata":{"id":"yoPZl6CF3DPQ","executionInfo":{"status":"ok","timestamp":1668597744262,"user_tz":-60,"elapsed":192,"user":{"displayName":"Gauthier Etien","userId":"01301474931245940039"}}},"outputs":[],"source":["from tensorflow.keras import layers\n","from tensorflow.keras import models\n","\n","\n","def initialize_model():\n","\n","    model = models.Sequential()\n","\n","    #model.add(layers.Reshape((28, 28, 1), input_shape=(28,28))),\n","    #model.add(layers.experimental.preprocessing.Rescaling(scale=1./255.)),\n","\n","    ### First Convolution & MaxPooling\n","    model.add(layers.Conv2D(8, (4,4), input_shape = (28,28,1), padding='same', activation=\"relu\")),\n","    model.add(layers.MaxPool2D(pool_size=(2,2))),\n","    \n","    ### Second Convolution & MaxPooling\n","    model.add(layers.Conv2D(16, (3,3), padding='same', activation=\"relu\")),\n","    model.add(layers.MaxPool2D(pool_size=(2,2))),\n","    \n","    ### Flattening\n","    model.add(layers.Flatten()),\n","    \n","    ### One Fully Connected layer - \"Fully Connected\" is equivalent to saying \"Dense\"\n","    model.add(layers.Dense(10, activation='relu')),\n","  #  layers.Dropout(0.3), #30% unchanged\n","    \n","    ### Last layer - Classification Layer with 10 outputs corresponding to 10 digits\n","    model.add(layers.Dense(10, activation='softmax'))\n","    \n","    ### Model compilation\n","    model.compile(loss='categorical_crossentropy', \n","              optimizer='adam',\n","              metrics=['accuracy'])\n","    return model\n","    \n","model = initialize_model()"]},{"cell_type":"markdown","metadata":{"id":"Yd83qt3G3DPQ"},"source":["‚ùì **Question: number of trainable parameters in a convolutional layer** ‚ùì \n","\n","How many trainable parameters are there in your model?\n","1. Compute them with ***model.summary( )*** first\n","2. Recompute them manually to make sure you properly understood ***what influences the number of weights in a CNN***."]},{"cell_type":"code","execution_count":80,"metadata":{"tags":["challengify"],"colab":{"base_uri":"https://localhost:8080/"},"id":"V8uiq6-e3DPQ","executionInfo":{"status":"ok","timestamp":1668597748823,"user_tz":-60,"elapsed":215,"user":{"displayName":"Gauthier Etien","userId":"01301474931245940039"}},"outputId":"66f28874-136b-444e-cb1a-bb45e2fc18b3"},"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"sequential_8\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," conv2d_16 (Conv2D)          (None, 28, 28, 8)         136       \n","                                                                 \n"," max_pooling2d_16 (MaxPoolin  (None, 14, 14, 8)        0         \n"," g2D)                                                            \n","                                                                 \n"," conv2d_17 (Conv2D)          (None, 14, 14, 16)        1168      \n","                                                                 \n"," max_pooling2d_17 (MaxPoolin  (None, 7, 7, 16)         0         \n"," g2D)                                                            \n","                                                                 \n"," flatten_8 (Flatten)         (None, 784)               0         \n","                                                                 \n"," dense_16 (Dense)            (None, 10)                7850      \n","                                                                 \n"," dense_17 (Dense)            (None, 10)                110       \n","                                                                 \n","=================================================================\n","Total params: 9,264\n","Trainable params: 9,264\n","Non-trainable params: 0\n","_________________________________________________________________\n"]}],"source":["model.summary()"]},{"cell_type":"markdown","metadata":{"id":"G7QCCMh13DPQ"},"source":["### (2.2) Training a CNN"]},{"cell_type":"markdown","metadata":{"id":"oQCvzSgD3DPR"},"source":["‚ùì **Question: training a CNN** ‚ùì \n","\n","Initialize your model and fit it on the train data. \n","- Do not forget to use a **Validation Set/Split** and an **Early Stopping criterion**. \n","- Limit yourself to 5 epochs max in this challenge, just to save some precious time for the more advanced challenges!"]},{"cell_type":"code","source":["X_train.shape\n","y_train_cat.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LIkKmoHQKIaw","executionInfo":{"status":"ok","timestamp":1668597470031,"user_tz":-60,"elapsed":377,"user":{"displayName":"Gauthier Etien","userId":"01301474931245940039"}},"outputId":"22cd8215-c8e7-4355-f4a5-36f0898dbe0b"},"execution_count":71,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(60000, 10)"]},"metadata":{},"execution_count":71}]},{"cell_type":"code","execution_count":82,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dozqARFK3DPR","executionInfo":{"status":"ok","timestamp":1668598280754,"user_tz":-60,"elapsed":167196,"user":{"displayName":"Gauthier Etien","userId":"01301474931245940039"}},"outputId":"b3ffa597-47c8-4590-e4a2-e9ac28d6cb6c"},"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/5\n","1873/1875 [============================>.] - ETA: 0s - loss: 0.0577 - accuracy: 0.9822"]},{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"]},{"output_type":"stream","name":"stdout","text":["\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r1875/1875 [==============================] - 33s 18ms/step - loss: 0.0577 - accuracy: 0.9822\n","Epoch 2/5\n","1875/1875 [==============================] - ETA: 0s - loss: 0.0509 - accuracy: 0.9843"]},{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"]},{"output_type":"stream","name":"stdout","text":["\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r1875/1875 [==============================] - 33s 18ms/step - loss: 0.0509 - accuracy: 0.9843\n","Epoch 3/5\n","1875/1875 [==============================] - ETA: 0s - loss: 0.0454 - accuracy: 0.9861"]},{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"]},{"output_type":"stream","name":"stdout","text":["\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r1875/1875 [==============================] - 33s 18ms/step - loss: 0.0454 - accuracy: 0.9861\n","Epoch 4/5\n","1873/1875 [============================>.] - ETA: 0s - loss: 0.0428 - accuracy: 0.9872"]},{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"]},{"output_type":"stream","name":"stdout","text":["\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r1875/1875 [==============================] - 34s 18ms/step - loss: 0.0428 - accuracy: 0.9872\n","Epoch 5/5\n","1873/1875 [============================>.] - ETA: 0s - loss: 0.0386 - accuracy: 0.9880"]},{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"]},{"output_type":"stream","name":"stdout","text":["\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r1875/1875 [==============================] - 33s 18ms/step - loss: 0.0385 - accuracy: 0.9880\n"]},{"output_type":"execute_result","data":{"text/plain":["<keras.callbacks.History at 0x7efefc535ad0>"]},"metadata":{},"execution_count":82}],"source":["from tensorflow.keras.callbacks import EarlyStopping\n","\n","\n","es = EarlyStopping(patience=1, restore_best_weights=True)\n","\n","model.fit(X_train, y_train_cat, \n","          epochs=5,  \n","          batch_size=32, \n","          verbose=1,\n","          callbacks=[es])"]},{"cell_type":"markdown","metadata":{"id":"iUS-3ZUD3DPR"},"source":["‚ùì **Question: How many iterations does the CNN perform per epoch** ‚ùì\n","\n","_Note: it has nothing to do with the fact that this is a CNN. This is related to the concept of forward/backward propagation already covered during the previous lecture on optimizers, fitting, and losses üòâ_"]},{"cell_type":"markdown","metadata":{"tags":["challengify"],"id":"ZFGtuM3v3DPR"},"source":["> YOUR ANSWER HERE"]},{"cell_type":"markdown","metadata":{"id":"7K_RHoLY3DPS"},"source":["<details>\n","    <summary><i>Answer</i></summary>\n","\n","With `verbose = 1` when fitting your model, you have access to crucial information about your training procedure.\n","    \n","Remember that we've just trained our CNN model on $60000$ training images\n","\n","If the chosen batch size is 32: \n","\n","* For each epoch, we have $ \\large \\lceil \\frac{60000}{32} \\rceil = 1875$ minibatches <br/>\n","* The _validation_split_ is equal to $0.3$ - which means that within one single epoch, there are:\n","    * $ \\lceil 1875 \\times (1 - 0.3) \\rceil = \\lceil 1312.5 \\rceil = 1313$ batches are used to compute the `train_loss` \n","    * $ 1875 - 1312 = 562 $ batches are used to compute the `val_loss`\n","    * **The parameters are updated 1313 times per epoch** as there are 1313 forward/backward propagations per epoch !!!\n","\n","\n","üëâ With so many updates of the weights within one epoch, you can understand why this CNN model converges even with a limited number of epochs.\n","\n","</details>    \n"]},{"cell_type":"markdown","metadata":{"id":"CwmOGfz_3DPS"},"source":["### (2.3) Evaluating its performance"]},{"cell_type":"markdown","metadata":{"id":"knN3By2k3DPS"},"source":["‚ùì **Question: Evaluating your CNN** ‚ùì \n","\n","What is your **`accuracy on the test set?`**"]},{"cell_type":"code","execution_count":84,"metadata":{"tags":["challengify"],"colab":{"base_uri":"https://localhost:8080/"},"id":"qqB9V7T73DPS","executionInfo":{"status":"ok","timestamp":1668598404315,"user_tz":-60,"elapsed":3405,"user":{"displayName":"Gauthier Etien","userId":"01301474931245940039"}},"outputId":"b7f37298-6466-4f9f-accc-32981952cf8e"},"outputs":[{"output_type":"stream","name":"stdout","text":["313/313 [==============================] - 3s 8ms/step - loss: 0.0426 - accuracy: 0.9868\n","[0.042585164308547974, 0.9868000149726868]\n"]}],"source":["print(model.evaluate(X_test, y_test_cat, verbose=1))\n"]},{"cell_type":"markdown","metadata":{"id":"yiqIPbZO3DPS"},"source":["üéâ You should already be impressed by your CNN skills! Reaching over 95% accuracy!\n","\n","üî• You solved what was a very hard problem 30 years ago with your own CNN."]},{"cell_type":"markdown","metadata":{"id":"Sbg8jNJi3DPT"},"source":["üèÅ **Congratulations!**\n","\n","üíæ Don't forget to `git add/commit/push` your notebook...\n","\n","üöÄ ... and move on to the next challenge!"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}